{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、确认分析目标，可以稍微延伸；\n",
    "    a.用户画像（4W1H描述使用场景）\n",
    "    b.好评/差评（可与其他变量如星级、时间、变体、点赞数等交叉分析）\n",
    "    c.购买动机（why，其他同上）\n",
    "    d.技巧：CoT、few-shot learning、实体识别、情感分析（摘要、抽象）等\n",
    "2、基于数据搭建流程；\n",
    "    a.streamlit + 云*\n",
    "    b.pyqt*\n",
    "    c.BI tool\n",
    "3、快速验证流程有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入大模型、情感分析模型、测试数据等\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "# client_ds = OpenAI(api_key=\"sk-c54b0f152ba1473cbfb3ef2b0b65878b\", base_url=\"https://api.deepseek.com/v1\").chat.completions\n",
    "client_ds = OpenAI(api_key=\"sk-c54b0f152ba1473cbfb3ef2b0b65878b\", base_url=\"https://api.deepseek.com/beta\").chat.completions # 8k输出长度\n",
    "# client_kimi = OpenAI(api_key=\"sk-cMpxKyHfnkW4Iwr9BWWbr0M2Rg6J1tiez1sQSTSbTILaIWUF\", base_url=\"https://api.moonshot.cn/v1\").chat.completions\n",
    "data1 = pd.read_excel('./B07TXYC6WD-US-Reviews-240910-73658.xlsx')\n",
    "data1.columns = ['type','vp','vine','title','review','translation','rating','thumbup','picture','video','url','timing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原数据长度： 50\n",
      "无空字段数据长度： 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:40,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标题处理的数量：5\n",
      "评论处理的数量：4\n",
      "最终处理数据长度：50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 用户画像分析\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "from translate import Translator\n",
    "translator = Translator(from_lang='autodetect', to_lang='en')\n",
    "\n",
    "## 数据预处理\n",
    "# 数据量是否充足\n",
    "# 部分非英文需要翻译\n",
    "# 无效数据\n",
    "\n",
    "def preprocessing(df):\n",
    "    df_new = df.copy()\n",
    "    print(\"原数据长度：\", len(df_new))\n",
    "\n",
    "    # 去除空字段\n",
    "    not_na_idx = df_new[\"review\"].notna()\n",
    "    df_new = df_new.loc[not_na_idx]\n",
    "    df_new.reset_index(drop=True, inplace=True)\n",
    "    print(\"无空字段数据长度：\", len(df_new))\n",
    "\n",
    "    # 翻译\n",
    "    title_list = df_new['title'].to_list()\n",
    "    review_list = df_new['review'].to_list()\n",
    "    title_list_trans = []\n",
    "    review_list_trans = []\n",
    "    title_trans_count = 0\n",
    "    review_trans_count = 0\n",
    "    title_error_idx = []\n",
    "    review_error_idx = []\n",
    "\n",
    "    # for idx, title_review in tqdm(enumerate(zip(title_list, review_list))):\n",
    "    #     title = title_review[0]\n",
    "    #     review = title_review[1]\n",
    "\n",
    "    for idx, row in tqdm(df_new.iterrows()): # 与多线程测试速度\n",
    "        title = row['title']\n",
    "        review = row['review']\n",
    "\n",
    "        try:\n",
    "            title_lang = detect(title)\n",
    "        except Exception as e:\n",
    "            print(f'索引：{idx}；类型：title；内容：{title}；错误：{e}')\n",
    "            title_error_idx.append(idx)\n",
    "            title_lang = None\n",
    "\n",
    "        if title_lang == 'en' or title_lang is None:\n",
    "            title_list_trans.append(title)\n",
    "        else:\n",
    "            title_trans = translator.translate(title)\n",
    "            if title_trans == \"PLEASE SELECT TWO DISTINCT LANGUAGES\":\n",
    "                title_list_trans.append(title)\n",
    "            else:\n",
    "                title_list_trans.append(title_trans)\n",
    "                title_trans_count += 1        \n",
    "\n",
    "        try:\n",
    "            review_lang = detect(review)\n",
    "        except Exception as e:\n",
    "            print(f'索引：{idx}出错；类型：review；内容：{review}；错误：{e}')\n",
    "            review_error_idx.append(idx)\n",
    "            review_lang = None\n",
    "\n",
    "        if review_lang == 'en' or review_lang is None:\n",
    "            review_list_trans.append(review)\n",
    "        else:\n",
    "            review_trans = translator.translate(review)\n",
    "            if review_trans == \"PLEASE SELECT TWO DISTINCT LANGUAGES\":\n",
    "                review_list_trans.append(review)\n",
    "            else:\n",
    "                review_list_trans.append(review_trans)\n",
    "                review_trans_count += 1    \n",
    "\n",
    "    print(f\"标题处理的数量：{title_trans_count}\")\n",
    "    print(f\"评论处理的数量：{review_trans_count}\")\n",
    "\n",
    "    df_new['title'] = title_list_trans\n",
    "    df_new['review'] = review_list_trans\n",
    "\n",
    "    df_new.drop(title_error_idx, inplace = True, axis = 0)\n",
    "    df_new.drop(review_error_idx, inplace = True, axis = 0)\n",
    "    df_new.reset_index(drop=True, inplace = True)\n",
    "    print(f\"最终处理数据长度：{len(df_new)}\")\n",
    "\n",
    "    return df_new\n",
    "\n",
    "data1_processed = preprocessing(data1.head(idx)) # 测试速度\n",
    "# data1_processed.to_excel('./data1_processed.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "699it [1:12:27,  6.22s/it]\n"
     ]
    }
   ],
   "source": [
    "## 画像形成/评论分析：大模型实体抽取4W1H和购买动机，计算比例\n",
    "# 什么时间购买和使用：生日、日常游玩还是睡前游玩等\n",
    "# 什么场景：在哪里使用？家中还是车上等；\n",
    "# 用户关注点以及如何使用：产品特性、教学工具等\n",
    "# 用户：谁用、谁买\n",
    "# how：如何使用、如何发现\n",
    "# 动机：礼物、教育等 ？\n",
    "# 评价：是否好评或差点，原因是什么？\n",
    "\n",
    "def get_entity(title, review):\n",
    "    # entity extraction\n",
    "    role_function = \"\"\"\n",
    "            You are a helpful expert for entity extraction, and are capable of performing the task below.\n",
    "            \n",
    "            You are required to extract key entities and summarize the text content from the Amazon product review. \n",
    "\n",
    "            Key entities include:\n",
    "            1. Product purchase time (e.g., around birthday, before/after holidays, etc.)\n",
    "            2. Usage scenario (e.g., birthday party, gift, workplace, etc.)\n",
    "            3. Target user (e.g., children, elderly, etc.)\n",
    "            4. Purchase motive (e.g., for education, as a gift)\n",
    "            5. Information channel for discovering the product (e.g., recommended by others, redirected from other websites, etc.)\n",
    "\n",
    "            If there is no corresponding entity in the text, return an empty value.\n",
    "\n",
    "            The summary of the text content should include:\n",
    "            1. User's evaluation (positive, negative or neutral)\n",
    "            2. Reason for the positive or negative review\n",
    "            3. Product features that the user focuses on (e.g., color, size, etc.)\n",
    "\n",
    "            Return the results in JSON format and lower case.\n",
    "\n",
    "            Example text:\n",
    "            \"I bought this toy before Christmas as a gift for my nephew. He had a lot of fun with it at his birthday party, especially liking its color and size. I discovered this product through a friend's recommendation.\"\n",
    "\n",
    "            JSON format return result example:\n",
    "            {\n",
    "            \"entities\": {\n",
    "                \"purchase time\": \"before Christmas\",\n",
    "                \"usage scenario\": \"birthday party\",\n",
    "                \"target user\": \"nephew\",\n",
    "                \"purchase motive\": \"as a gift\",\n",
    "                \"information channel\": \"friend's recommendation\"\n",
    "            },\n",
    "            \"summary\": {\n",
    "                \"evaluation\": \"positive\",\n",
    "                \"reason\": \"nephew had a lot of fun at the birthday party\",\n",
    "                \"focus on product features\": \"color and size\"\n",
    "            }\n",
    "            }\n",
    "            \"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": role_function},\n",
    "                {\"role\": \"user\", \"content\": f\"Please process the following review: title - {title}; review - {review}\"}]\n",
    "    response = client_ds.create(\n",
    "                                model = \"deepseek-chat\",\n",
    "                                messages = messages,\n",
    "                                temperature = 0.1,\n",
    "                                response_format={'type':\"json_object\"})\n",
    "    entity_set = json.loads(response.choices[0].message.content)\n",
    "    return entity_set\n",
    "\n",
    "\n",
    "# idx = np.random.choice(data1_processed.index)\n",
    "# title = data1_processed['title'][idx]\n",
    "# review = data1_processed['review'][idx]\n",
    "# print(title)\n",
    "# print(review)\n",
    "# review_analysis = get_entity(title, review)\n",
    "# review_analysis\n",
    "\n",
    "def get_entity_for_all(data):\n",
    "    title_list = data['title'].to_list()\n",
    "    review_list = data['review'].to_list()\n",
    "\n",
    "    review_analysis = []\n",
    "    error_index = []    \n",
    "    for idx, title_review in tqdm(enumerate(zip(title_list, review_list))):\n",
    "        title = title_review[0]\n",
    "        review = title_review[1]\n",
    "\n",
    "        try:\n",
    "            result = get_entity(title, review)\n",
    "            review_analysis.append(result)\n",
    "        except Exception as e:\n",
    "            print(f'索引：{idx}出错；内容：{title} {review}；错误：{e}')\n",
    "            review_analysis.append({})\n",
    "            error_index.append(idx)\n",
    "        \n",
    "    return review_analysis, error_index\n",
    "\n",
    "\n",
    "review_analysis, error_index = get_entity_for_all(data1_processed)\n",
    "\n",
    "#### json验证\n",
    "#### 并发询问\n",
    "\n",
    "# import json\n",
    "# with open('product1_review_test_entity.json', 'w', encoding='utf-8') as json_file:\n",
    "#     for i in review_analysis:\n",
    "#         json.dump(i, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# data1_processed['review_analysis'] = review_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./product1_review_test_entity.json\", 'r', encoding='utf-8') as file:\n",
    "    review_analysis = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据长度：699\n",
      "长度：time 25、scenario 93、user 427、motive 139、channel 10、evaluation 698、reason 697、feature 484\n"
     ]
    }
   ],
   "source": [
    "# statistical analysis\n",
    "def analyze_entity(entity_set):\n",
    "    # 实体分析\n",
    "    time = []\n",
    "    scenario = []\n",
    "    user = []\n",
    "    motive = []\n",
    "    channel = []\n",
    "    evaluation = []\n",
    "    reason = []\n",
    "    feature = []\n",
    "    for subset in entity_set: #### 修改为列表\n",
    "        entities = subset['entities']\n",
    "        summary = subset['summary']\n",
    "        time.append(entities['purchase time'])\n",
    "        scenario.append(entities['usage scenario'])\n",
    "        user.append(entities['target user'])\n",
    "        motive.append(entities['purchase motive'])\n",
    "        channel.append(entities['information channel'])\n",
    "        evaluation.append(summary['evaluation'])\n",
    "        reason.append(summary['reason'])\n",
    "        feature.append(summary['focus on product features'])    \n",
    "    \n",
    "    ## 简单分析：每类实体的分布情况/基于模型总结 #### 加入评价分类、时间、退货\n",
    "    entity_set_list = [time,scenario,user,motive,channel,evaluation,reason,feature]\n",
    "    entity_set_filter = [[item for item in entity_type if len(item) > 0] for entity_type in entity_set_list]\n",
    "    print(f\"数据长度：{len(entity_set)}\")\n",
    "    print(f\"长度：time {len(entity_set_filter[0])}、scenario {len(entity_set_filter[1])}、user {len(entity_set_filter[2])}、motive {len(entity_set_filter[3])}、\"\\\n",
    "        f\"channel {len(entity_set_filter[4])}、evaluation {len(entity_set_filter[5])}、reason {len(entity_set_filter[6])}、feature {len(entity_set_filter[7])}\")\n",
    "    \n",
    "    return entity_set_filter, evaluation, reason\n",
    "    \n",
    "\n",
    "    #### 如何减少计算量\n",
    "    ## 交叉分析：实体的交互情况\n",
    "    ## 星级、变体、点赞数\n",
    "\n",
    "entity_set_filter, evaluation, reason = analyze_entity(review_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 回归数据本身\n",
    "# from collections import Counter\n",
    "# specific_entity = entity_set_filter[1]\n",
    "# print(Counter(specific_entity))\n",
    "# sorted(specific_entity)\n",
    "\n",
    "## 需要做统一\n",
    "# 分类（意思相近、长相类似，而不是基于属性） -> 计算\n",
    "## jaccard算法：基于token\n",
    "### 需要聚类：time、scenario、user、motive、reason、feature\n",
    "### 直接展示：channel、evaluation\n",
    "### 选取中心词：随机 or 大模型拟定（evaluation）\n",
    "## 计算：每个属性的每个聚类占比 + 文字描述即可\n",
    "\n",
    "import string\n",
    "from copy import deepcopy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_punct(text):\n",
    "    # 使用string.punctuation获取所有英文标点符号\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # 去除标点符号\n",
    "    text_without_punct = text.translate(translator)\n",
    "    return text_without_punct\n",
    "\n",
    "def remove_stopwords(ls):\n",
    "    return [item for item in ls if item not in stop_words]\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    set1 = set(set1)\n",
    "    set2 = set(set2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return intersection / union\n",
    "\n",
    "def group_entity(entity_set, threshold=0.5):\n",
    "    entity_set = deepcopy(entity_set)\n",
    "    if len(entity_set) == 0:\n",
    "        return {}\n",
    "    else:\n",
    "        # 去除标点符号\n",
    "        entity_set_clean = list(map(remove_punct, entity_set))\n",
    "        # 分词\n",
    "        entity_set_token = list(map(word_tokenize, entity_set_clean))\n",
    "        # 去除停用词\n",
    "        entity_set_no_stopwords = list(map(remove_stopwords, entity_set_token))\n",
    "        # 形成对应关系\n",
    "        # entity_set_dict = dict(zip(entity_set, entity_set_no_stopwords))\n",
    "\n",
    "        # 分类/中心词选定\n",
    "                    \n",
    "    return entity_set_no_stopwords\n",
    "\n",
    "# specific_entity_group = group_entity(specific_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1: set of books\n",
      "Phrases 1: ['set of books', 'number of books, photo representation', 'country of origin and washing instructions', 'lots of pages', 'number of books and washing instructions', 'content and instructional nature', 'softness, durability, and ease of washing', 'sturdiness, ease of cleaning', 'colour and value', 'sensory perception, selling technique', 'color, worth of money', 'vivid color, drawing of different animals, fabric tails with various textures', 'description and price', 'quality, machine washable, attachment to paci clip', 'bright colours and clear illustrations', 'quantity and size', 'colors, tails security', 'tails of each animal', 'value for the money', 'durability, noise level', 'textures, noises, and colors', 'colours and educational content about animals', 'quality, product description, wording, pages with crinkle', 'colorful, good quality', 'quality and color', 'tails sticking out from each page, crinkly pages, ties in the center of the book, quality of construction, colors, wording and design', 'colors and crinkly pages', 'quantity and price', 'colorful textured materials, strings for tying', 'sounds and visual stimulation', 'crinkles, different feelings on the tails, colorful', 'number of books, price', 'colors, pictures, tails, crinkly texture', 'chew, crackle, hold, and snuffle', 'cute appearance, number of books', 'price and quality', 'form and material', 'cover texture, inside pages texture, number of books', 'noise, durable material, colours, textured tails', 'originality', 'size and additional activities', 'allows for chewing, pulling, squashing', 'size of the books', 'sound effects, tail wagging page turners, bookmark', 'colorful textured animals, tails, squeaker, velvety soft pages, crinkle pages', 'durability and engagement', 'crinkles, pages are bright', 'quantity of books', 'construction quality, color, functionality, and safety instructions', 'cute crinkle book, well made, darling', 'cleanliness and color', 'softness, names for dinosaurs', 'fabric of the book, softness', 'quantity, clarity of product description, and price', 'crinkle fabric, Velcro part', 'bright colours and perfect for tiny hands', 'tails, number of books', 'fabric, contrasts', 'detail of the animals', 'color and fabrics', 'size and image', 'cuteness and baby friendliness', 'texture, sound', 'crinkle, noise-making', 'cuteness, price, washability', 'crinkly cover, color', 'tails, cover, cloth ties, washability', 'quantity and condition', 'cute, crinkles, vibrant', 'price and misleading picture', 'number of books received', 'price, number of books', 'value for money, quality', 'hygiene level, quality', 'number of books, cloth softness', 'size and quality', 'number of books', 'beauty and niceness', 'tails and texture of the crinkle pages', 'washable, good colors', 'material', 'number of books and pictures', 'crinkle noise, ease of holding', 'easy to handle, sensory features like a squeaker and crinkly paper, tails easy for baby to grab', 'dimensions and number of pages', 'soft and colorful', 'tails, crinkly pages, brightness, cheerfulness', 'bright colors, interesting crinkle noise', 'content and educational value', 'dimensions, construction, presentation', 'color and size', 'size and material', 'price, quantity', 'noise and tails', 'lightness, softness, crinkly material', 'order of the book', 'number of tails and squeaky button', 'colourful, crinkly with tails, ties to attach to pram or car seat', 'completeness of the book', 'visual and tactile', 'colors, crinkle pages', 'quality material, different fabrics, textures', 'cute appearance and tactile elements', 'shapes, colors, texture, noise', 'cute and many things to feel and play with', 'quality, advertisement', 'tails sticking out of the sides, crinkly sound', 'cost and quality', 'colorful, crinkle sound', 'material/stuffing, bright colors, cute tails', 'fabric and colors', 'well made, makes noises', 'graphics and colors', 'unique concept', 'material and price', 'crinkling noise, different tails, durability, ease of cleaning', 'quantity (one book), size, and price', 'price and advertisement clarity', 'colorful book, crinkle feature for teething infant', 'names of the animals', 'brightly colored and tactile', 'durability and cuteness', 'crinkly texture, bright colors, grabable tails', 'fuzzy tails and bright colors', 'visual appeal, tactile feedback, auditory feedback', 'color and busy design', 'animal tails, softness, crunchy sound', 'textures, crinkle sound material, machine washable, size', 'colors, tactile feel, portability', 'colors, animals', 'cover material', 'quality and design', 'sounds and texture', 'size and price', 'washability, safety for babies', 'vibrant colours, different shapes, fabric quality', 'soft material, crinkliness, noise, and colors', 'quality, price, advertisement clarity, and choice of book', 'crinkle sounds, tails, bookmarks', 'polyester fabric, crackling sound, brightly coloured, soft tails, internal press button, non-toxic, washable, no sharp edges', 'color, different animals, textured tails, crinkly sounding', 'color, tactile and auditory aspects', 'quality and description', 'bright colors, story text, material of the tails and pages', 'quality, number of books', 'teething and crinkling', 'color, beauty, fabric', 'size, appearance, smell', 'color, noises', 'number of books included', 'tails and noises', 'bright colors and things that make noise', 'pictures and colors, easy to hold', 'textured tails, crinkly, colorful, hidden sound button', 'crinkly, soft, colorful, touch and feel', 'sensory and motor skills', 'number of pages, crinkle feature, chemical smell', 'quality and different animal tails', 'cute, soft, one book', 'tails and printing', 'quality, price', 'crunches when touched, different feels on each tail, two red ties', 'price, number of pages', 'illustrations, durability', 'colors and durability', 'colour, texture, softness', 'price and size', 'packaging, presentation, fabric pages', 'chew satisfaction and crinkle distraction', 'softness, chewable', 'cute, expensive, washable', 'price and number of pages', 'size and interactivity', 'tails, design', 'size, interactivity', 'crunchy, colorful, texture', 'crinkle noise, different textures, tails', 'textures, squeaker']\n",
      "=================================\n",
      "Group 2: material hanging off\n",
      "Phrases 2: ['material hanging off', 'crinkly sound and things hanging off the pages', 'colorful, easy to hold and turn pages, little tails sticking out from different animals', 'sticking out parts', 'crinkle sound on front and back cover', 'ease of fitting on a bag']\n",
      "=================================\n",
      "Group 3: different colors and materials, crinkly pages, stuffed tails, squeaker\n",
      "Phrases 3: ['different colors and materials, crinkly pages, stuffed tails, squeaker', 'water friendly and easy read', 'tails and noise', 'softness, durability', 'durability and washability', 'colorful, creative, animal tails sticking out', 'crinkle pages, different textures, tie strings', 'washable, bite-resistant, simple letters, big pictures', 'colorful, cute, attention grabbing', 'textures, colors, sounds, tail size', 'representation and description', 'bright colours and patterns', 'pages, tails, colors', 'crinkly cover page, simple animal pictures', 'materials, models for touch', 'tails and book', 'colors, textures, animals, language', 'softness, color', 'price, description, and images', 'quality and noise', 'tails and crinkle sound', 'sturdiness, simplicity, variety', 'size, colour', 'crinkle, sounds', 'colours, texts', 'colors, fabric', 'bright colors, tails', 'content and value', 'colorful and sensory factor', 'washability, crinkly sound', 'brightly coloured, soft tails, crackling sound, press button sound, educational content, non-toxic fabric, washable, no sharp edges', 'quality, quantity, advertisement clarity', 'colors, tails, pages, and chewing', 'color and tails', 'quality and textures', 'color, texture', 'tails, specifically the tiger one', 'colors, size, durability', 'different textures, length, squeaky toy', 'touchy feely and sounds', 'cuteness and washability', 'crinkle sounds, washability', 'cute, sounds', 'tails, rustling sound', 'sound and colors', 'crinkle books, tiger and monkey tails', 'quality, quantity', 'colour and texture', 'size, quality', 'crumble sound, tails to grab and chew', 'size, crinkly pages, textured tails, bright colors', 'price and quantity', 'material, weight', 'size, space for tails, colors, materials', 'colorful and sturdy', 'book, long red strings', 'tails, quality', 'price, size, appearance', 'colorful, crunchy, story', 'possibilities to use as a toy, motor training', 'sound it makes, chewing/sucking toy', 'noise, tails', 'colors and 3d tails', 'colors and fun sounds', 'feel and sounds', 'feel and crinkly noises', 'cute and noisy', 'textures']\n",
      "=================================\n",
      "Group 4: words\n",
      "Phrases 4: ['words']\n",
      "=================================\n",
      "Group 5: sound part\n",
      "Phrases 5: ['sound part']\n",
      "=================================\n",
      "Group 6: sensory experience\n",
      "Phrases 6: ['sensory experience', 'tactile', 'sensory things']\n",
      "=================================\n",
      "Group 7: group pictured\n",
      "Phrases 7: ['group pictured']\n",
      "=================================\n",
      "Group 8: cute little book\n",
      "Phrases 8: ['cute little book', 'book', 'washable book', 'cute books', 'cute book', 'little book', 'book quality']\n",
      "=================================\n",
      "Group 9: books\n",
      "Phrases 9: ['books', 'first books']\n",
      "=================================\n",
      "Group 10: soft touches\n",
      "Phrases 10: ['soft touches', 'soft fabric tails', 'fabric', 'soft baby cloth books', 'texture', 'soft pages']\n",
      "=================================\n",
      "Group 11: jungly tails\n",
      "Phrases 11: ['jungly tails', 'animal tails', 'tails', 'crocodile tail material', 'tails squeak']\n",
      "=================================\n",
      "Group 12: ability to manipulate\n",
      "Phrases 12: ['ability to manipulate', 'soft and easy to hold', 'easy to clean']\n",
      "=================================\n",
      "Group 13: crinkly toy/books\n",
      "Phrases 13: ['crinkly toy/books']\n",
      "=================================\n",
      "Group 14: tone\n",
      "Phrases 14: ['tone']\n",
      "=================================\n",
      "Group 15: price, size\n",
      "Phrases 15: ['price, size', 'price comparison', 'price']\n",
      "=================================\n",
      "Group 16: chewability\n",
      "Phrases 16: ['chewability']\n",
      "=================================\n",
      "Group 17: colour\n",
      "Phrases 17: ['colour', 'color', 'colors', 'vibrant colors']\n",
      "=================================\n",
      "Group 18: thickness\n",
      "Phrases 18: ['thickness']\n",
      "=================================\n",
      "Group 19: crinkley\n",
      "Phrases 19: ['crinkley']\n",
      "=================================\n",
      "Group 20: cuteness\n",
      "Phrases 20: ['cuteness']\n",
      "=================================\n",
      "Group 21: picture representation\n",
      "Phrases 21: ['picture representation']\n",
      "=================================\n",
      "Group 22: machine washable\n",
      "Phrases 22: ['machine washable', 'packaging', 'chewable', 'washable']\n",
      "=================================\n",
      "Group 23: touchy feely\n",
      "Phrases 23: ['touchy feely']\n",
      "=================================\n",
      "Group 24: crinkle feature\n",
      "Phrases 24: ['crinkle feature', 'features']\n",
      "=================================\n",
      "Group 25: cute\n",
      "Phrases 25: ['cute']\n",
      "=================================\n",
      "Group 26: value for money\n",
      "Phrases 26: ['value for money']\n",
      "=================================\n",
      "Group 27: crinkles sounds\n",
      "Phrases 27: ['crinkles sounds', 'crinkle', 'crinkle sound']\n",
      "=================================\n",
      "Group 28: cute, holds baby’s attention\n",
      "Phrases 28: ['cute, holds baby’s attention']\n",
      "=================================\n",
      "Group 29: crinkly or crunchy sound\n",
      "Phrases 29: ['crinkly or crunchy sound']\n",
      "=================================\n",
      "Group 30: safety\n",
      "Phrases 30: ['safety']\n",
      "=================================\n",
      "Group 31: tails are all different\n",
      "Phrases 31: ['tails are all different']\n",
      "=================================\n",
      "Group 32: crinkle pages\n",
      "Phrases 32: ['crinkle pages', 'crinkle page', 'pages crinkle', 'crinkly pages']\n",
      "=================================\n",
      "Group 33: looks like multiple cute books\n",
      "Phrases 33: ['looks like multiple cute books']\n",
      "=================================\n",
      "Group 34: tea set\n",
      "Phrases 34: ['tea set']\n",
      "=================================\n",
      "Group 35: title\n",
      "Phrases 35: ['title']\n",
      "=================================\n",
      "Group 36: sound when touched\n",
      "Phrases 36: ['sound when touched']\n",
      "=================================\n",
      "Group 37: Bibi device\n",
      "Phrases 37: ['Bibi device']\n",
      "=================================\n",
      "Group 38: appearance\n",
      "Phrases 38: ['appearance']\n",
      "=================================\n",
      "Group 39: washability\n",
      "Phrases 39: ['washability', 'durability', 'quality', 'cleanability', 'sturdiness']\n",
      "=================================\n",
      "Group 40: photo\n",
      "Phrases 40: ['photo']\n",
      "=================================\n",
      "Group 41: language\n",
      "Phrases 41: ['language']\n",
      "=================================\n",
      "Group 42: brightness\n",
      "Phrases 42: ['brightness']\n",
      "=================================\n",
      "Group 43: colorful and fun\n",
      "Phrases 43: ['colorful and fun']\n",
      "=================================\n",
      "Group 44: well made\n",
      "Phrases 44: ['well made']\n",
      "=================================\n",
      "Group 45: SWR (Standing Wave Ratio)\n",
      "Phrases 45: ['SWR (Standing Wave Ratio)']\n",
      "=================================\n",
      "Group 46: softness\n",
      "Phrases 46: ['softness']\n",
      "=================================\n",
      "Group 47: size\n",
      "Phrases 47: ['size']\n",
      "=================================\n",
      "Group 48: crinkly\n",
      "Phrases 48: ['crinkly']\n",
      "=================================\n",
      "Group 49: noise\n",
      "Phrases 49: ['noise']\n",
      "=================================\n",
      "Group 50: quantity\n",
      "Phrases 50: ['quantity']\n",
      "=================================\n",
      "Group 51: animals\n",
      "Phrases 51: ['animals']\n",
      "=================================\n",
      "Group 52: look\n",
      "Phrases 52: ['look']\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    # 计算点积\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    \n",
    "    # 计算向量的模\n",
    "    norm_a = np.linalg.norm(vector_a)\n",
    "    norm_b = np.linalg.norm(vector_b)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    similarity = dot_product / (norm_a * norm_b)                                                                            \n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# 加载spaCy模型\n",
    "## 不同模型（sm、md）\n",
    "## 不同阈值（0.5、0.6、0.7、0.8、0.9）\n",
    "## 速度尚可、效果不行 time（聚类效果可能还不如jaccard）、scenario（效果也不好）、user（不太适合统一）、motive（形式太多不适合统一）、reason（形式太多不适合统一）、feature（聚类效果不好）\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "specific_entity = list(set(entity_set_filter[-1]))\n",
    "threshold = 0.7\n",
    "\n",
    "def aggregate_similar_phrases(phrases, threshold=0.5):\n",
    "    # 批量转换\n",
    "    docs = list(map(nlp, phrases))\n",
    "    phase_docs = dict(zip(phrases, docs))\n",
    "\n",
    "    # 创建一个字典来存储聚合结果\n",
    "    aggregated_phrases_temp = defaultdict(dict)\n",
    "\n",
    "    # 遍历每个短语\n",
    "    for phrase, doc in phase_docs.items():\n",
    "        # 初始化一个标志来表示是否找到匹配\n",
    "        found_match = False\n",
    "\n",
    "        # 遍历已有的聚合组\n",
    "        for key, group in aggregated_phrases_temp.items():\n",
    "            # 计算当前短语与聚合组中第一个短语的相似度\n",
    "            similarity = cosine_similarity(doc.vector, group['avg_vec']) # 组平均向量作为标准\n",
    "\n",
    "            # 如果相似度超过某个阈值（例如0.8），则认为它们是相似的\n",
    "            if similarity >=  threshold:\n",
    "                group[phrase] = doc.vector\n",
    "                group['avg_vec'] = sum(y for x, y in group.items() if x != 'avg_vec')/(len(group) - 1) # 更新组平均向量\n",
    "                found_match = True\n",
    "                break\n",
    "\n",
    "        # 如果没有找到匹配，则创建一个新的聚合组\n",
    "        if not found_match:\n",
    "            aggregated_phrases_temp[phrase][phrase] = doc.vector\n",
    "            aggregated_phrases_temp[phrase]['avg_vec'] = doc.vector\n",
    "    \n",
    "    keys = list(aggregated_phrases_temp.keys())\n",
    "    groups = [[__ for __ in _ if __ != 'avg_vec'] for _ in aggregated_phrases_temp.values()]\n",
    "    aggregated_phrases = dict(zip(keys, groups))\n",
    "\n",
    "    return aggregated_phrases\n",
    "\n",
    "aggregated_phrases = aggregate_similar_phrases(specific_entity, threshold=threshold)\n",
    "\n",
    "# 打印聚合结果\n",
    "for idx, key_group in enumerate(aggregated_phrases.items()):\n",
    "    print(rf\"Group {idx+1}: {key_group[0]}\")\n",
    "    print(rf\"Phrases {idx+1}: {key_group[1]}\")\n",
    "    print(rf\"=================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非去重总量：484\n",
      "实际总量：339\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 473 column 6 (char 17247)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[354], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m specific_entity_nodup \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(specific_entity))\n\u001b[0;32m     83\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m实际总量：\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(specific_entity_nodup)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m entity_cluster \u001b[39m=\u001b[39m get_cluster(\u001b[39m\"\u001b[39;49m\u001b[39m购买时间\u001b[39;49m\u001b[39m\"\u001b[39;49m, specific_entity_nodup)\n\u001b[0;32m     85\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m统计总量：\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39msum\u001b[39m([i[\u001b[39m'\u001b[39m\u001b[39m数量\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mi\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mentity_cluster\u001b[39m.\u001b[39mvalues()])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m entity_cluster\n",
      "Cell \u001b[1;32mIn[354], line 70\u001b[0m, in \u001b[0;36mget_cluster\u001b[1;34m(name, entitty_set)\u001b[0m\n\u001b[0;32m     63\u001b[0m messages \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: role_function},\n\u001b[0;32m     64\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m以下是某款产品购买用户的\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m，大概有\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(entitty_set)\u001b[39m}\u001b[39;00m\u001b[39m个，请按照你的角色功能设定对以下一系列描述进行处理：\u001b[39m\u001b[39m{\u001b[39;00menetity_set_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m}]\n\u001b[0;32m     65\u001b[0m response \u001b[39m=\u001b[39m client_ds\u001b[39m.\u001b[39mcreate(\n\u001b[0;32m     66\u001b[0m                             model \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdeepseek-chat\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m                             messages \u001b[39m=\u001b[39m messages,\n\u001b[0;32m     68\u001b[0m                             temperature \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m,\n\u001b[0;32m     69\u001b[0m                             response_format\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mjson_object\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m---> 70\u001b[0m entity_set \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(response\u001b[39m.\u001b[39;49mchoices[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mmessage\u001b[39m.\u001b[39;49mcontent)\n\u001b[0;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m entity_set\n",
      "File \u001b[1;32md:\\miniconda\\envs\\rag\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32md:\\miniconda\\envs\\rag\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32md:\\miniconda\\envs\\rag\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 473 column 6 (char 17247)"
     ]
    }
   ],
   "source": [
    "def get_cluster(name, entitty_set):\n",
    "    # entity clustering\n",
    "    enetity_set_str = \"、\".join(entitty_set)\n",
    "    role_function = '''\n",
    "                    角色说明：\n",
    "                        你是一个擅长数据分析的数据分析师，现在你需要完成以下任务。\n",
    "\n",
    "                    任务说明：\n",
    "                        我会给你某一产品用户购买特征的一系列描述（如，购买时间、使用场景等），请将相似的描述归类，并统计每个类别的数量。\n",
    "                    \n",
    "                    操作步骤：\n",
    "                    1. 阅读并理解每条描述。\n",
    "                    2. 根据描述内容，识别并归类相似的描述。\n",
    "                    3. 对每个类别进行计数。\n",
    "                    4. 输出每个类别、对应的数量以及数个例子。\n",
    "\n",
    "                    注意事项：\n",
    "                    1. 归类时主要考虑描述的相似性（比如说“1st birthday”和“for his 1st birthday”可以归为“生日”类别，“the number of books”和“quantity”可以归类为商品数量）。\n",
    "                    2. 类别名称不能过于笼统，比如不能出现“书籍特征”这种类别，因为无法从“书籍特征”中得知具体的书籍特征。\n",
    "                    3. 输出的例子个数为2~10个左右，注意不能重复输出。\n",
    "                    4. 统计时注意不要遗漏或重复计数，总量要与输入一致。\n",
    "                    5. 返回json格式。  \n",
    "\n",
    "                    示例输入：\n",
    "                            ```                                                \n",
    "                            shower,\n",
    "                            a few months ago,\n",
    "                            for his 1st birthday,\n",
    "                            during Easter2023,\n",
    "                            Easter,\n",
    "                            first Christmas,\n",
    "                            for Christmas,\n",
    "                            when baby #2 was 4 months and daughter was 2yrs,\n",
    "                            baby’s first Christmas,\n",
    "                            a few days ago,\n",
    "                            since Christmas,\n",
    "                            at Christmas,\n",
    "                            birthday,\n",
    "                            for a Christmas present,\n",
    "                            for Christmas,\n",
    "                            early Christmas,\n",
    "                            1st birthday,\n",
    "                            1st birthday,\n",
    "                            1st birthday,\n",
    "                            Christmas,\n",
    "                            1st birthday,\n",
    "                            Christmas,\n",
    "                            for Christmas,\n",
    "                            when she was 5 1/2 months old,\n",
    "                            when he was about 3 months\n",
    "                            ```\n",
    "\n",
    "                    示例输出：\n",
    "                            ```\n",
    "                            {\n",
    "                            \"生日\": {\"数量\":7,\"例子\":[\"1st birthday\",\"birthday\",...]}, \n",
    "                            \"圣诞节\": {\"数量\":10,\"例子\":[\"for Christmas\",\"Christmas\",...]},\n",
    "                            \"宝宝成长\": {\"数量\":2,\"例子\":[\"when he was about 3 months\",\"when she was 5 1/2 months old\"]},\n",
    "                            \"万圣节\": {\"数量\":1,\"例子\":[\"Easter\"]}\n",
    "                            }\n",
    "                            ```\n",
    "            '''\n",
    "    messages = [{\"role\": \"system\", \"content\": role_function},\n",
    "                {\"role\": \"user\", \"content\": f\"以下是某款产品购买用户的{name}，大概有{len(entitty_set)}个，请按照你的角色功能设定对以下一系列描述进行处理：{enetity_set_str}\"}]\n",
    "    response = client_ds.create(\n",
    "                                model = \"deepseek-chat\",\n",
    "                                messages = messages,\n",
    "                                temperature = 0.5,\n",
    "                                response_format={'type':\"json_object\"})\n",
    "    entity_set = json.loads(response.choices[0].message.content)\n",
    "    return entity_set\n",
    "\n",
    "# 购买时间：数量少，所以分类和数量基本准确\n",
    "# 使用场景：数量少，所以分类和数量基本准确\n",
    "# 0\\1\n",
    "idx = -1\n",
    "specific_entity = entity_set_filter[idx]\n",
    "print(f\"非去重总量：{len(specific_entity)}\")\n",
    "specific_entity_nodup = list(set(specific_entity))\n",
    "print(f\"实际总量：{len(specific_entity_nodup)}\")\n",
    "entity_cluster = get_cluster(\"购买时间\", specific_entity_nodup)\n",
    "print(f\"统计总量：{sum([i['数量'] for i in entity_cluster.values()])}\")\n",
    "entity_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "非处理数据总量：139\n",
      "非去重后实体总量：226\n",
      "数量统计： {'Christmas': 1, 'Grandmas': 1, 'add': 1, 'alternative': 1, 'animals': 1, 'another': 1, 'baby': 12, 'back': 1, 'best': 1, 'birthday': 1, 'book': 4, 'books': 2, 'buying': 1, 'category': 1, 'charity': 1, 'children': 2, 'colour': 1, 'could': 1, 'coworker': 1, 'daughter': 1, 'develop': 1, 'development': 2, 'developmental': 1, 'dinosaurs': 1, 'early': 1, 'easily': 1, 'education': 5, 'entertainment': 1, 'first': 1, 'following': 1, 'free': 1, 'friend': 1, 'friends': 1, 'get': 1, 'gift': 90, 'gifted': 1, 'gifts': 4, 'give': 1, 'good': 2, 'got': 1, 'great': 3, 'habit': 1, 'hands': 1, 'house': 1, 'intended': 1, 'interested': 1, 'items': 1, 'learn': 1, 'learning': 2, 'listed': 2, 'little': 2, 'looking': 1, 'loved': 1, 'makes': 1, 'mom': 1, 'much': 1, 'needed': 1, 'needs': 1, 'new': 1, 'obsessed': 1, 'one': 2, 'outgrew': 1, 'part': 1, 'personal': 1, 'play': 1, 'present': 6, 'purposes': 1, 'reading': 2, 'registry': 3, 'reviews': 1, 'road': 1, 'safe': 1, 'seller': 1, 'sensory': 2, 'sent': 1, 'shower': 5, 'small': 2, 'son': 1, 'stimulation': 1, 'surgery': 1, 'texture': 1, 'thing': 1, 'thought': 2, 'toy': 2, 'trip': 1, 'use': 2, 'washed': 1, 'would': 1, '’': 1}\n",
      "去重后实体总量：89\n"
     ]
    }
   ],
   "source": [
    "# 用户类别：数量正确，直接对名词（形式）归类即可\n",
    "# 产品关注点：数量不正确，直接对名词（形式）归类即可\n",
    "# 标点符号、tokenization、stopwords、提示词 \n",
    "# 2\\-1\n",
    "idx = 3\n",
    "specific_entity = entity_set_filter[idx]\n",
    "print(f\"非处理数据总量：{len(specific_entity)}\")\n",
    "specific_entity_nopunct = list(map(remove_punct, specific_entity))\n",
    "specific_entity_token = list(map(word_tokenize, specific_entity_nopunct))\n",
    "specific_entity_nostopwords = list(map(remove_stopwords, specific_entity_token)) #### stopword可以优化 words = [token.text for token in doc if not token.is_stop]\n",
    "specific_entity_full = []\n",
    "for x in specific_entity_nostopwords:\n",
    "    for y in x:\n",
    "        specific_entity_full.append(y)\n",
    "specific_entity_full = sorted(specific_entity_full)\n",
    "specific_entity_full\n",
    "specific_entity_sorted = dict(Counter(specific_entity_full))\n",
    "print(f\"非去重后实体总量：{len(specific_entity_full)}\")\n",
    "print(\"数量统计：\", specific_entity_sorted)\n",
    "print(f\"去重后实体总量：{len(set(specific_entity_full))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实体类别：礼物、教育和发展、个人使用、其他\n",
      "非去重后实体总量：139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'礼物': {'gift': 25,\n",
       "  'as a gift': 48,\n",
       "  'give as a gift': 1,\n",
       "  'makes a great gift': 1,\n",
       "  'gifts': 1,\n",
       "  'gifted to a new mom': 1,\n",
       "  'for a baby shower gift': 1,\n",
       "  'as little Christmas gifts': 1,\n",
       "  'baby gift': 2,\n",
       "  'for gifts': 1,\n",
       "  'as a baby shower gift': 1,\n",
       "  'as a small gift': 1,\n",
       "  'gift for early development': 1,\n",
       "  'to add to some baby shower gifts': 1,\n",
       "  'for a present': 2,\n",
       "  'as a birthday present': 1,\n",
       "  'for a gift': 3,\n",
       "  'as a present': 2,\n",
       "  'great gift': 2,\n",
       "  'as a baby gift': 1,\n",
       "  'part of a gift': 1,\n",
       "  'intended gift': 1,\n",
       "  'gift or personal use': 1},\n",
       " '教育和发展': {'for developmental purposes': 1,\n",
       "  'for sensory development': 1,\n",
       "  'for sensory books that could be easily washed': 1,\n",
       "  'for education': 5,\n",
       "  'learning toy book': 1,\n",
       "  'to learn the animals': 1,\n",
       "  'to develop a good reading habit': 1},\n",
       " '个人使用': {'thought daughter outgrew it, got it for road trip': 1,\n",
       "  'son is obsessed with dinosaurs': 1,\n",
       "  'thought this book would be just the thing': 1,\n",
       "  'safe alternative': 1,\n",
       "  'following the good reviews': 1,\n",
       "  'as a toy': 1,\n",
       "  'needs stimulation by colour and texture items': 1,\n",
       "  'for small children': 1,\n",
       "  'for a baby who is learning to use their little hands': 1,\n",
       "  'because she loved them so much': 1,\n",
       "  'for reading': 1,\n",
       "  \"needed one at Grandma's house\": 1,\n",
       "  'buying another one': 1,\n",
       "  'for baby': 1,\n",
       "  'to get children interested in books': 1,\n",
       "  'looking for a book': 1,\n",
       "  'sent with a friend': 1,\n",
       "  'for entertainment': 1,\n",
       "  'to play with after a back surgery': 1},\n",
       " '其他': {'because its a best seller in baby book category': 1,\n",
       "  'listed on a coworker’s baby registry': 1,\n",
       "  'as a shower present': 1,\n",
       "  'for a baby shower': 1,\n",
       "  'registry': 1,\n",
       "  'for friends that are having their first baby': 1,\n",
       "  'listed on her registry': 1,\n",
       "  'free': 1,\n",
       "  'for charity': 1}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cluster(name, entitty_set_count):\n",
    "    # entity clustering\n",
    "    role_function = '''\n",
    "                    角色说明：\n",
    "                        你是一个擅长文字理解和数据分析的专家，现在你需要完成以下任务。\n",
    "\n",
    "                    任务说明：\n",
    "                        我会给你从某一产品的用户评论中抽取出来和购买特征相关的一系列关键词（如用户类别以及用户对产品的关注点等）以及统计数量，我需要你把形式上和语义上相似的关键词进一步归类并返回。\n",
    "                    \n",
    "                    操作步骤：\n",
    "                    1. 阅读并理解每个关键词。\n",
    "                    2. 根据关键词形式上和语义上的相似度进行归类。\n",
    "                    3. 输出每个类别以及该类别的关键词和数量。\n",
    "\n",
    "                    注意事项：\n",
    "                    1. 归类时主要考虑描述的形似和语义相似性（比如说“quantity”和“number”可以归为“数量”类别，“babies”、“baby”和“infant”可以归为“婴儿”类别）。\n",
    "                    2. 并且根据我给出的关键词的属性对关键词序列进行筛选，忽略无关的关键词。比如，我告诉你关键词属于“用户的类别”，关键词序列为\n",
    "                    （{'baby': 150, 'old': 76, 'month': 44, 'babies': 38, 'grandson': 29, 'little': 25}），的“old”、“month”、“little”可以忽略（因为这些数量词性质上不是用户）只需要返回“baby”、“babies”、“grandson”\n",
    "                    3. 保证返回的关键词和统计数量与输入一致，不能出现原序列里不存在的关键词。\n",
    "                    4. 以json格式返回。  \n",
    "\n",
    "                    示例输入：\n",
    "                            ```\n",
    "                            关键词属性：用户对产品的关注点\n",
    "                            关键词序列：                                                \n",
    "                                {'tails': 68,\n",
    "                                'books': 50,\n",
    "                                'price': 50,\n",
    "                                'quality': 48,\n",
    "                                'quantity': 48,\n",
    "                                'colors': 37,\n",
    "                                'size': 37,\n",
    "                                'number': 34,\n",
    "                                'crinkle': 28,\n",
    "                                'pages': 28,\n",
    "                                'color': 23,\n",
    "                                'crinkly': 23,\n",
    "                                'cute': 23,\n",
    "                                'sound': 22,\n",
    "                                'book': 19,\n",
    "                                'durability': 18,\n",
    "                                'colorful': 17,\n",
    "                                'different': 15,\n",
    "                                'material': 15,\n",
    "                                ...}\n",
    "                            ```\n",
    "\n",
    "                    示例输出：\n",
    "                            其中，关键词“different”与属性“用户对产品的关注点”无关，则直接忽略\n",
    "                            ```\n",
    "                            {\n",
    "                            \"书籍\": {\"books\":50, \"book\":19},\n",
    "                            \"颜色\": {\"colors\":37, \"colorful\":17},\n",
    "                            \"材质\": {\"crinkly\":23, \"crinkle\":28, \"material\":15},\n",
    "                            \"数量\": {\"number\":34, \"quantity\":48},\n",
    "                            \"质量\": {\"quality\":48, \"durability\":18},\n",
    "                            \"声音\": {\"sound\": 22, \"durability\":18},\n",
    "                            \"可爱外表\": {\"cute\": 23, \"tails\":68},\n",
    "                            \"尺寸\": {\"size\": 37, \"durability\":18},\n",
    "                            \"价格\": {\"price\": 50},\n",
    "                            ...\n",
    "                            }\n",
    "                            ```\n",
    "            '''\n",
    "    messages = [{\"role\": \"system\", \"content\": role_function},\n",
    "                {\"role\": \"user\", \"content\": f\"以下序列是某款产品的{name}（关键词属性），请按照你的角色功能设定进行处理：{entitty_set_count}（关键词序列）\"}]\n",
    "    response = client_ds.create(\n",
    "                                model = \"deepseek-chat\",\n",
    "                                messages = messages,\n",
    "                                temperature = 0.1,\n",
    "                                response_format={'type':\"json_object\"})\n",
    "    entity_set = json.loads(response.choices[0].message.content)\n",
    "    return entity_set\n",
    "\n",
    "# 用户的类别：分类基本准确，数量基本准确 \n",
    "# 用户对产品的关注点：分类基本准确，作图最好加上原评论辅助理解，数量基本正确\n",
    "\n",
    "entity_cluster = get_cluster(\"用户购买动机\", specific_entity_sorted)\n",
    "count = 0\n",
    "for x, y in entity_cluster.items():\n",
    "    for name, num in y.items():\n",
    "        count += num\n",
    "print(\"实体类别：\"+\"、\".join(entity_cluster.keys()))\n",
    "print(f\"非去重后实体总量：{count}\")\n",
    "entity_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motive（形式太多不适合统一；和上面两个实体同样的聚类方法；3）、reason（形式太多不适合统一）\n",
    "## reason：先总结要点、然后分类、然后再统计\n",
    "\n",
    "# evaluation_reasons = \"\"\n",
    "evaluation_reasons = []\n",
    "for idx, pair in enumerate(zip(evaluation, reason)):\n",
    "    # evaluation_reasons += f'{idx+1}. evluation: {pair[0]}, reason: {pair[1]}\\n'\n",
    "    evaluation_reasons.append(f'evluation: {pair[0]}, reason: {pair[1]}')\n",
    "\n",
    "import json\n",
    "with open(\"./product1_review_test_pos_neg.json\", 'r', encoding='utf-8') as file:\n",
    "    reason_points = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "699it [15:10,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# 总结要求\n",
    "def get_point(evaluation_reasons):\n",
    "    role_function = r'''\n",
    "                    角色说明：\n",
    "                        你是一个擅长文字总结的专家，现在你需要完成以下任务。\n",
    "\n",
    "                    任务说明：\n",
    "                        我会给你一系列从某一产品的用户评论中抽取出来用户对这款产品的好评、差评、中性以及对应原因，请你简洁地用自己的语言总结出这些评论的好评点和差评点。\n",
    "\n",
    "                    操作步骤：\n",
    "                    1. 遍历阅读并理解每个评论的属性（好评、差评和中性）以及对应原因。\n",
    "                    2. 总结出差评点和好评点各有哪些，数量尽量精简，并给出1~3个简短的例子。\n",
    "\n",
    "                    注意事项：\n",
    "                    1. 不能随意捏造与输入事实不符的内容。\n",
    "                    2. 以json格式返回好评点/差评点。\n",
    "\n",
    "                    示例输入：\n",
    "                            ```\n",
    "                            1. evluation: positive, reason: crinkle sounds and tails keep the kiddo entertained, but the reviewer is confused about the bookmarks\n",
    "                            2. evluation: positive, reason: absolutely adorable and great quality\n",
    "                            3. evluation: negative, reason: expected a book but received a limp cloth item that resembled a chew toy\n",
    "                            4. evluation: positive, reason: child loves to play with the product\n",
    "                            5. evluation: positive, reason: the user found the product nice\n",
    "                            6. evluation: positive, reason: the user simply states 'GOOD'\n",
    "                            7. evluation: positive, reason: daughter loves the crinkling noise and holding on to all the different tails, very durable, easy to clean\n",
    "                            8. evluation: positive, reason: mommy to be is very excited for her baby to have it\n",
    "                            9. evluation: negative, reason: quality of the product is not very good and there should have been something inside that made other kind of noise\n",
    "                            10. evluation: negative, reason: misleading picture and overpriced\n",
    "                            ...\n",
    "                            ```\n",
    "\n",
    "                    示例输出：\n",
    "                            ```\n",
    "                            {\n",
    "                            \"postive point\": [\n",
    "                                            \"entertaining\": [\"crinkling\", \"loved by kids\"],\n",
    "                                            \"good quality\": [\"cleanability\", \"durability\"],\n",
    "                                            ...\n",
    "                                            ],\n",
    "                            \"negative point\": [\n",
    "                                            \"misleading package\": [\"bookmark confusion\", \"misleading picture\"],\n",
    "                                            \"unreasonable price\": [\"overpriced\", \"durability\"],\n",
    "                                            ...\n",
    "                                            ]\n",
    "                            }\n",
    "                            ```\n",
    "            '''\n",
    "    \n",
    "                            # {\n",
    "                            # \"postive pointe\": {\n",
    "                            #                 \"entertainment\": \"crinkle sounds and tails keep the kiddo entertained\",\n",
    "                            #                 \"great quality\": \"very durable\",\n",
    "                            #                 ...\n",
    "                            #                 },\n",
    "                            # \"negative point\": {\n",
    "                            #                 \"misleading appearance\": \"expected a book but received a limp cloth item that resembled a chew toy\",\n",
    "                            #                 \"unreasonable price\" \"overpricd\",\n",
    "                            #                 \"poor quality\": \"quality of the product is not very good\",\n",
    "                            #                 ...\n",
    "                            #                 }\n",
    "                            # }\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": role_function},\n",
    "                {\"role\": \"user\", \"content\": f\"以下是一系列评论属性以及对应原因：{evaluation_reasons}，请按照你的角色功能进行处理。\"}]\n",
    "    response = client_ds.create(\n",
    "                                model = \"deepseek-chat\",\n",
    "                                messages = messages,\n",
    "                                temperature = 0.3,\n",
    "                                response_format={'type':\"json_object\"})\n",
    "    entity_set = json.loads(response.choices[0].message.content)\n",
    "    return entity_set\n",
    "\n",
    "# grouped_points = get_point(evaluation_reasons)\n",
    "\n",
    "# 分类\n",
    "def get_point(evaluation_reason, reason_points):\n",
    "    role_function = r'''\n",
    "                    角色设定：\n",
    "                        你是一位专业的文本分析专家，擅长从用户评论中提取和对比关键信息。\n",
    "\n",
    "                    任务说明：\n",
    "                        分析给定的用户评论，并将其与提供的好评点和差评点进行对比，以确定评论中包含的是好评点还是差评点。\n",
    "\n",
    "                    操作步骤：\n",
    "                        1. 仔细阅读并理解用户提供的真实评论和评价。\n",
    "                        2. 将真实评论与好评点和差评点（以及示例）进行对比分析。\n",
    "                        3. 根据分析结果，确定评论中包含的好评点或差评点。\n",
    "\n",
    "                    注意事项：\n",
    "                        1. 确保输出结果基于输入内容，不得编造与事实不符的信息。\n",
    "                        2. 以JSON格式返回分析结果，指出评论中的好评点或差评点。\n",
    "\n",
    "                    示例输入：\n",
    "                            ```\n",
    "                            好坏评点和示例：\n",
    "                                    {'positive point': [{'entertaining': ['crinkling',\n",
    "                                                            'loved by kids',\n",
    "                                                            \"baby loves the crinkle noise and it's easy to hold\",\n",
    "                                                            'baby loves the crinkle page',\n",
    "                                                            'baby loves the crinkle sound']},\n",
    "                                    {'good quality': ['cleanability',\n",
    "                                                            'durability',\n",
    "                                                            'well made',\n",
    "                                                            'excellent quality',\n",
    "                                                            'good quality fabric that should stand up to rough handling, chewing and multiple washes']}],\n",
    "                                    'negative point': [{'misleading package': ['misleading picture',\n",
    "                                                            'misleading description',\n",
    "                                                            'misleading product description',\n",
    "                                                            'misleading information about the number of books included for the price',\n",
    "                                                            'misleading photo showing five books']},\n",
    "                                    {'unreasonable price': ['overpriced',\n",
    "                                                            'expensive for one book',\n",
    "                                                            'not worth the price',\n",
    "                                                            'high price for one book',\n",
    "                                                            'expensive for just one item']}]}\n",
    "                            真实评论：\n",
    "                                    evluation: negative, reason: not worth the money\n",
    "                            ```\n",
    "\n",
    "                    示例输出：\n",
    "                            ```\n",
    "                            {\"point\": ['unreasonable price']}\n",
    "                            ```\n",
    "            '''\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": role_function},\n",
    "                {\"role\": \"user\", \"content\": f\"以下是一条用户评价和评论：{evaluation_reason}以及好评点和差评点{reason_points}，请按照你的角色功能进行处理。\"}]\n",
    "    response = client_ds.create(\n",
    "                                model = \"deepseek-chat\",\n",
    "                                messages = messages,\n",
    "                                temperature = 0.1,\n",
    "                                response_format={'type':\"json_object\"})\n",
    "    points = json.loads(response.choices[0].message.content)\n",
    "    return points\n",
    "\n",
    "import numpy as np\n",
    "# idx = np.random.choice(list(range(len(evaluation_reasons))))\n",
    "# evaluation_reason = evaluation_reasons[idx]\n",
    "# points = get_point(evaluation_reason, reason_points)\n",
    "# print(evaluation_reason)\n",
    "# print(points)\n",
    "\n",
    "# 统计\n",
    "from tqdm import tqdm \n",
    "def get_statistics(evaluation_reasons, reason_points):\n",
    "    points = []\n",
    "    for idx, evaluation_reason in tqdm(enumerate(evaluation_reasons)):\n",
    "        try:\n",
    "            point = get_point(evaluation_reason, reason_points)\n",
    "            points.append(point['point'])\n",
    "        except Exception as e:\n",
    "            points.append([])\n",
    "            print(f\"索引：{idx}；内容：{evaluation_reason}；出错原因：{e}\")\n",
    "    return points\n",
    "points = get_statistics(evaluation_reasons, reason_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'entertaining': 229,\n",
       "         'engaging features': 175,\n",
       "         'misleading package': 143,\n",
       "         'good quality': 139,\n",
       "         'unreasonable price': 115,\n",
       "         'educational value': 84,\n",
       "         'interactive': 59,\n",
       "         'poor quality': 52,\n",
       "         'safety concerns': 11,\n",
       "         'meets expectations': 1,\n",
       "         'user loves the product': 1,\n",
       "         'recipient was very pleased with the little book': 1,\n",
       "         'product is recommended': 1,\n",
       "         'ideal baby gift': 1,\n",
       "         'parents seem pleased': 1})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_list = []\n",
    "for x in points:\n",
    "    for y in x:\n",
    "        points_list.append(y)\n",
    "from collections import Counter\n",
    "Counter(points_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
